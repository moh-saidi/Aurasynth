{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8cdebff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T16:38:44.723300Z",
     "iopub.status.busy": "2025-05-17T16:38:44.723075Z",
     "iopub.status.idle": "2025-05-17T16:38:44.731331Z",
     "shell.execute_reply": "2025-05-17T16:38:44.730475Z"
    },
    "papermill": {
     "duration": 0.019298,
     "end_time": "2025-05-17T16:38:44.732265",
     "exception": true,
     "start_time": "2025-05-17T16:38:44.712967",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '’' (U+2019) (3642017967.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_19/3642017967.py\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    //Le modèle apprend ainsi à générer des compositions musicales cohérentes et expressives à partir d’instructions en langage naturel.\u001b[0m\n\u001b[0m                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '’' (U+2019)\n"
     ]
    }
   ],
   "source": [
    "//Fonctionnement du Modèle\n",
    "//L'application repose sur un modèle de diffusion discrète pour générer des séquences MIDI à partir d'instructions textuelles. Le pipeline se compose de trois modules principaux :\n",
    "\n",
    "//Encodage sémantique (FLAN-T5) :\n",
    "//Une instruction en langage naturel (ex. : \"une mélodie joyeuse en do majeur\") est encodée par FLAN-T5, un modèle de langage pré-entraîné, pour capturer son sens musical et contextuel.\n",
    "\n",
    "//Diffusion discrète :\n",
    "//Une séquence MIDI tokenisée (via un tokenizer REMI) est progressivement corrompue par un processus de bruitage selon un cosine beta schedule. Cela transforme la musique en une version bruitée difficile à reconnaître.\n",
    "\n",
    "//Denoising par Transformer :\n",
    "//Un Transformer sert de denoiseur et tente de reconstruire la séquence originale étape par étape. Il est conditionné sur les embeddings de FLAN-T5, guidant ainsi la génération musicale selon le texte initial.\n",
    "\n",
    "//Le modèle apprend ainsi à générer des compositions musicales cohérentes et expressives à partir d’instructions en langage naturel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876fe01d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "/// USING THE SAME DATASET USED BY AMAAI-Lab TO AVOID RE CLEANING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ecf0a",
   "metadata": {
    "id": "rHhbQThszrKu",
    "outputId": "5302f963-543d-4b0c-dedf-a154c4fbaba1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/AMAAI-Lab/Text2midi.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46844895",
   "metadata": {
    "id": "MSaD41sp0tau",
    "outputId": "3195f8d2-0042-4d91-d7f3-213f8156c34a",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/amaai-lab/MidiCaps/resolve/main/midicaps.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bfc3e1",
   "metadata": {
    "id": "UFeAN3qA1CMz",
    "outputId": "694e3b9c-a802-4a8d-da29-843396c2f1d5",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def decompress_tar_gz(file_path, extract_path=\".\"):\n",
    "    if file_path.endswith(\".tar.gz\"):\n",
    "        with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=extract_path)\n",
    "            print(f\"Extracted to: {os.path.abspath(extract_path)}\")\n",
    "    else:\n",
    "        print(\"The file is not a .tar.gz archive.\")\n",
    "\n",
    "# Example usage\n",
    "decompress_tar_gz(\"midicaps.tar.gz\", \"output_directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed12bd",
   "metadata": {
    "id": "MlZJtocK2FsQ",
    "outputId": "73c4570f-3fcd-499d-c49f-1a2c6d1620f7",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers accelerate miditok wandb spacy jsonlines pyyaml tqdm nltk\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a90c9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "src = \"/kaggle/working/Text2midi/captions/captions.json\"\n",
    "dst = \"/kaggle/working/captions.json\"\n",
    "\n",
    "# Move the file\n",
    "shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2d8b6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "directory = '/kaggle/working/'\n",
    "\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "\n",
    "    if filename.startswith(\"checkpoint_epoch_\") and filename.endswith(\".bin\"):\n",
    "        try:\n",
    "\n",
    "            epoch = int(filename.split('_')[2].split('.')[0])\n",
    "            \n",
    "\n",
    "            if 5 <= epoch <= 50:\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bad437",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODE = \"build_vocab_remi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd1341",
   "metadata": {
    "id": "La3yqg383PLO",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def _get_activation_fn(activation: str):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    raise RuntimeError(f\"activation should be relu/gelu, not {activation}\")\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"Cosine schedule as proposed in https://arxiv.org/abs/2102.09672\"\"\"\n",
    "    steps = torch.arange(timesteps, dtype=torch.float32)\n",
    "    f_t = torch.cos(((steps / timesteps + s) / (1.0 + s) * math.pi / 2)) ** 2\n",
    "    betas = 1.0 - f_t / torch.roll(f_t, shifts=1, dims=0)\n",
    "    betas = torch.clamp(betas, 0.0, 0.999)\n",
    "    betas[0] = 0.0001  # Avoid zero beta\n",
    "    return betas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c9613",
   "metadata": {
    "id": "lqfj3irC3T_-",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4: Model Components\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class DiscreteDiffusionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, dim_feedforward=2048,\n",
    "                 num_steps=1000, dropout=0.1, device=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.time_emb = nn.Embedding(num_steps, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=5000).to(device)\n",
    "\n",
    "        # FLAN-T5 encoder\n",
    "        self.encoder = T5EncoderModel.from_pretrained(\"google/flan-t5-base\").to(device)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Project FLAN-T5 output (768) to d_model (512)\n",
    "        self.text_projection = nn.Linear(768, d_model).to(device)\n",
    "\n",
    "        # Transformer-based denoiser\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True, activation='gelu'\n",
    "        )\n",
    "        self.denoiser = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output layer\n",
    "        self.projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # D3PM: Uniform transition matrix\n",
    "        self.Q = torch.ones(vocab_size, vocab_size, device=device) / vocab_size\n",
    "        self.Q_bar = torch.ones(num_steps, vocab_size, vocab_size, device=device)\n",
    "        self.log_Q = torch.log(self.Q + 1e-10)\n",
    "\n",
    "        # Noise schedule (cosine)\n",
    "        self.betas = cosine_beta_schedule(num_steps).to(device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bar = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "        # Precompute Q_bar for each timestep\n",
    "        for t in range(num_steps):\n",
    "            alpha_bar_t = self.alpha_bar[t]\n",
    "            self.Q_bar[t] = alpha_bar_t * torch.eye(vocab_size, device=device) + \\\n",
    "                           (1 - alpha_bar_t) * self.Q\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def forward(self, x_t, t, src, src_mask):\n",
    "        # x_t: Noisy tokens (batch, seq_len)\n",
    "        # t: Timestep (batch,)\n",
    "        # src: Text input (batch, text_len)\n",
    "        # src_mask: Text attention mask (batch, text_len)\n",
    "        x_emb = self.token_emb(x_t) * math.sqrt(self.d_model)\n",
    "        x_emb = self.pos_encoder(x_emb.transpose(0, 1)).transpose(0, 1)\n",
    "        t_emb = self.time_emb(t).unsqueeze(1)  # (batch, 1, d_model)\n",
    "        x_emb = x_emb + t_emb\n",
    "        memory = self.encoder(src, attention_mask=src_mask).last_hidden_state  # (batch, text_len, 768)\n",
    "        memory = self.text_projection(memory)  # (batch, text_len, 512)\n",
    "        output = self.denoiser(x_emb, memory, memory_mask=None)\n",
    "        logits = self.projection(output)\n",
    "        return logits\n",
    "\n",
    "    def sample(self, src, src_mask, seq_len, num_steps=None, ddim=False, eta=0.0):\n",
    "      device = src.device\n",
    "      batch_size = src.size(0)\n",
    "      x_t = torch.randint(0, self.vocab_size, (batch_size, seq_len), device=device)\n",
    "      num_steps = num_steps or self.num_steps\n",
    "\n",
    "      if ddim:\n",
    "          step_indices = torch.linspace(0, self.num_steps - 1, steps=self.num_steps // num_steps + 1, device=device).long()\n",
    "      else:\n",
    "          step_indices = torch.arange(num_steps, device=device)\n",
    "\n",
    "      for i in reversed(range(len(step_indices))):\n",
    "          t = step_indices[i]\n",
    "          t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "          with torch.no_grad():\n",
    "              logits = self(x_t, t_tensor, src, src_mask)\n",
    "\n",
    "          # Debugging to see the shape of logits\n",
    "          # print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "          # Ensure logits have shape [batch_size, seq_len, vocab_size]\n",
    "          if len(logits.shape) == 3:  # [batch_size, seq_len, vocab_size]\n",
    "              # We need to reshape for multinomial sampling\n",
    "              # Flatten batch_size and seq_len dimensions\n",
    "              batch_seq_size = logits.size(0) * logits.size(1)\n",
    "              vocab_size = logits.size(2)\n",
    "\n",
    "              # Reshape to [batch_size*seq_len, vocab_size]\n",
    "              flat_logits = logits.reshape(batch_seq_size, vocab_size)\n",
    "              probs = F.softmax(flat_logits, dim=-1)\n",
    "\n",
    "              # Sample from the flattened distribution\n",
    "              flat_samples = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "              # Reshape back to [batch_size, seq_len]\n",
    "              x_t_new = flat_samples.reshape(logits.size(0), logits.size(1))\n",
    "          else:\n",
    "              # If logits are already 2D [batch_size, vocab_size]\n",
    "              probs = F.softmax(logits, dim=-1)\n",
    "              x_t_new = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "          if ddim and i > 0:\n",
    "              t_prev = step_indices[i - 1]\n",
    "              alpha_bar_t = self.alpha_bar[t]\n",
    "              alpha_bar_t_prev = self.alpha_bar[t_prev]\n",
    "              pred_x0 = x_t_new  # Use the new sampled tokens\n",
    "              sigma = eta * torch.sqrt((1 - alpha_bar_t_prev) / (1 - alpha_bar_t) * (1 - alpha_bar_t / alpha_bar_t_prev))\n",
    "              x_t = torch.where(\n",
    "                  torch.rand_like(x_t.float()) < sigma,\n",
    "                  torch.randint(0, self.vocab_size, x_t.shape, device=device),\n",
    "                  pred_x0\n",
    "              )\n",
    "          else:\n",
    "              x_t = x_t_new\n",
    "\n",
    "      return x_t\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0555c5b",
   "metadata": {
    "id": "tIy6KbdN3Ys_",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5: Dataset\n",
    "class Text2MusicDataset(Dataset):\n",
    "    def __init__(self, configs, captions, remi_tokenizer, mode=\"train\", shuffle=False):\n",
    "        self.mode = mode\n",
    "        self.captions = captions\n",
    "        if shuffle:\n",
    "            random.shuffle(self.captions)\n",
    "        self.dataset_path = configs['raw_data']['dataset_folder']\n",
    "        self.remi_tokenizer = remi_tokenizer\n",
    "        self.nlp = English()\n",
    "        self.nlp.add_pipe('sentencizer')\n",
    "        self.t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        self.decoder_max_sequence_length = configs['model']['text2midi_model']['decoder_max_sequence_length']\n",
    "        self.num_steps = configs['model']['text2midi_model'].get('num_diffusion_steps', 1000)\n",
    "        self.vocab_size = len(remi_tokenizer)\n",
    "\n",
    "        # Noise schedule\n",
    "        self.betas = cosine_beta_schedule(self.num_steps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bar = torch.cumprod(self.alphas, dim=0)\n",
    "        self.Q = torch.ones(self.vocab_size, self.vocab_size) / self.vocab_size\n",
    "\n",
    "        print(\"Length of dataset:\", len(self.captions))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.captions[idx]['caption']\n",
    "        midi_filepath = os.path.join(self.dataset_path, self.captions[idx]['location'])\n",
    "\n",
    "        if not os.path.exists(midi_filepath):\n",
    "            raise FileNotFoundError(f\"MIDI file not found: {midi_filepath}\")\n",
    "\n",
    "        try:\n",
    "            tokens = self.remi_tokenizer(midi_filepath)\n",
    "            tokenized_midi = ([self.remi_tokenizer[\"BOS_None\"]] + tokens.ids +\n",
    "                             [self.remi_tokenizer[\"EOS_None\"]]) if tokens.ids else [\n",
    "                             self.remi_tokenizer[\"BOS_None\"], self.remi_tokenizer[\"EOS_None\"]]\n",
    "            tokenized_midi = torch.tensor(tokenized_midi)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error tokenizing MIDI file {midi_filepath}: {str(e)}\")\n",
    "\n",
    "        if len(tokenized_midi) < self.decoder_max_sequence_length:\n",
    "            x_0 = F.pad(tokenized_midi, (0, self.decoder_max_sequence_length - len(tokenized_midi))).long()\n",
    "        else:\n",
    "            x_0 = tokenized_midi[:self.decoder_max_sequence_length].long()\n",
    "\n",
    "        t = torch.randint(0, self.num_steps, (1,)).item()\n",
    "        alpha_bar_t = self.alpha_bar[t]\n",
    "        Q_t = alpha_bar_t * torch.eye(self.vocab_size) + (1 - alpha_bar_t) * self.Q\n",
    "        probs = F.one_hot(x_0, num_classes=self.vocab_size).float() @ Q_t\n",
    "        x_t = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "        if random.random() > 0.5 and self.mode == \"train\":\n",
    "            sentences = list(self.nlp(caption).sents)\n",
    "            if sentences:\n",
    "                sent_length = len(sentences)\n",
    "                drop_pct = (20 + random.random() * 30) / 100\n",
    "                how_many_to_drop = int(np.floor(drop_pct * sent_length) if sent_length < 4 else np.ceil(drop_pct * sent_length))\n",
    "                which_to_drop = np.random.choice(sent_length, how_many_to_drop, replace=False)\n",
    "                new_sentences = [s for i, s in enumerate(sentences) if i not in which_to_drop]\n",
    "                new_sentences = \" \".join([s.text for s in new_sentences])\n",
    "            else:\n",
    "                new_sentences = caption\n",
    "        else:\n",
    "            new_sentences = caption\n",
    "\n",
    "        inputs = self.t5_tokenizer(new_sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "        return (inputs['input_ids'].squeeze(0), inputs['attention_mask'].squeeze(0),\n",
    "                x_t.long(), t, x_0.long())\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = nn.utils.rnn.pad_sequence([item[0] for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = nn.utils.rnn.pad_sequence([item[1] for item in batch], batch_first=True, padding_value=0)\n",
    "    x_t = nn.utils.rnn.pad_sequence([item[2] for item in batch], batch_first=True, padding_value=0)\n",
    "    t = torch.tensor([item[3] for item in batch])\n",
    "    x_0 = nn.utils.rnn.pad_sequence([item[4] for item in batch], batch_first=True, padding_value=0)\n",
    "    return input_ids, attention_mask, x_t, t, x_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d0cfb",
   "metadata": {
    "id": "EopfV7fk3fIv",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_midi(model, tokenizer, t5_tokenizer, caption, seq_len, output_dir, device, ddim=True):\n",
    "    model.eval()\n",
    "    inputs = t5_tokenizer(caption, return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tokens = model.sample(input_ids, attention_mask, seq_len, ddim=ddim)\n",
    "    \n",
    "    token_ids = tokens[0].cpu().numpy().tolist()\n",
    "    if token_ids[0] == tokenizer[\"BOS_None\"]:\n",
    "        token_ids = token_ids[1:]\n",
    "    if token_ids[-1] == tokenizer[\"EOS_None\"]:\n",
    "        token_ids = token_ids[:-1]\n",
    "    \n",
    "    try:\n",
    "        # Use decode instead of tokens_to_midi\n",
    "        midi_score = tokenizer.decode(token_ids)\n",
    "        output_path = os.path.join(output_dir, f\"generated_{int(time.time())}.mid\")\n",
    "        \n",
    "        # Use dump_midi method which is available in the ScoreTick object\n",
    "        midi_score.dump_midi(output_path)\n",
    "                \n",
    "        print(f\"Generated MIDI saved to {output_path}\")\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting tokens to MIDI: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a0486",
   "metadata": {
    "id": "KtI2DqB-3jzN",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 8: Vocabulary Building\n",
    "\n",
    "import datetime\n",
    "\n",
    "def build_vocab(configs):\n",
    "    vocab = {}\n",
    "    for i in INSTRUMENTS:\n",
    "        vocab[('prefix', 'instrument', i)] = len(vocab) + 1\n",
    "    velocity = [0, 15, 30, 45, 60, 75, 90, 105, 120, 127]\n",
    "    midi_pitch = list(range(0, 128))\n",
    "    onset = list(range(0, 5001, 10))\n",
    "    duration = list(range(0, 5001, 10))\n",
    "    for v in velocity:\n",
    "        for i in INSTRUMENTS:\n",
    "            for p in midi_pitch:\n",
    "                if i != \"drum\":\n",
    "                    vocab[(i, p, v)] = len(vocab) + 1\n",
    "    for p in midi_pitch:\n",
    "        vocab[(\"drum\", p)] = len(vocab) + 1\n",
    "    for o in onset:\n",
    "        vocab[(\"onset\", o)] = len(vocab) + 1\n",
    "    for d in duration:\n",
    "        vocab[(\"dur\", d)] = len(vocab) + 1\n",
    "    special_tokens = [\"<T>\", \"<D>\", \"<U>\", \"<SS>\", \"<S>\", \"<E>\", \"SEP\"]\n",
    "    for token in special_tokens:\n",
    "        vocab[token] = len(vocab) + 1\n",
    "    print(f\"Vocabulary length: {len(vocab)}\")\n",
    "    vocab_path = os.path.join(configs[\"artifact_folder\"], \"vocab.pkl\")\n",
    "    os.makedirs(configs[\"artifact_folder\"], exist_ok=True)\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    print(f\"Vocabulary saved to {vocab_path}\")\n",
    "\n",
    "def build_vocab_remi(configs):\n",
    "    BEAT_RES = {(0, 1): 12, (1, 2): 4, (2, 4): 2, (4, 8): 1}\n",
    "    TOKENIZER_PARAMS = {\n",
    "        \"pitch_range\": (21, 109),\n",
    "        \"beat_res\": BEAT_RES,\n",
    "        \"num_velocities\": 32,\n",
    "        \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n",
    "        \"use_chords\": False,\n",
    "        \"use_rests\": False,\n",
    "        \"use_tempos\": True,\n",
    "        \"use_time_signatures\": True,\n",
    "        \"use_programs\": True,\n",
    "        \"num_tempos\": 32,\n",
    "        \"tempo_range\": (40, 250),\n",
    "    }\n",
    "    config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "    tokenizer = REMI(config)\n",
    "    caption_path = \"/kaggle/working/captions.json\"\n",
    "    if not os.path.exists(caption_path):\n",
    "        raise FileNotFoundError(f\"Caption file not found: {caption_path}\")\n",
    "    with jsonlines.open(caption_path) as reader:\n",
    "        captions = list(reader)\n",
    "    midi_paths = [os.path.join(configs['raw_data']['dataset_folder'],\n",
    "                  captions[i]['location']) for i in range(len(captions))][:30000]\n",
    "    for path in midi_paths:\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"MIDI file not found: {path}\")\n",
    "    print(f\"Vocabulary length: {tokenizer.vocab_size}\")\n",
    "    vocab_path = os.path.join(configs[\"artifact_folder\"], \"vocab_remi.pkl\")\n",
    "    os.makedirs(configs[\"artifact_folder\"], exist_ok=True)\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(f\"Vocabulary saved to {vocab_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497ba5d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODE = \"build_vocab_remi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66b337",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Cell 9: Execute Mode\n",
    "if MODE == \"train\":\n",
    "    train_model_accelerate(CONFIG)\n",
    "elif MODE == \"build_vocab\":\n",
    "    build_vocab(CONFIG)\n",
    "elif MODE == \"build_vocab_remi\":\n",
    "    build_vocab_remi(CONFIG)\n",
    "else:\n",
    "    raise ValueError(f\"Invalid mode: {MODE}. Choose 'train', 'build_vocab', or 'build_vocab_remi'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac5f12",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODE = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e6eb3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install mido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73720f61",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mido import MidiFile, MidiTrack, Message\n",
    "\n",
    "dummy = MidiFile()\n",
    "track = MidiTrack()\n",
    "dummy.tracks.append(track)\n",
    "\n",
    "# Add a single dummy note\n",
    "track.append(Message('note_on', note=60, velocity=64, time=0))\n",
    "track.append(Message('note_off', note=60, velocity=64, time=480))\n",
    "\n",
    "dummy.save('/kaggle/working/output_directory/lmd_full/9/9d762ce1f025b6df8e87335092024626.mid')\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a7e64",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"wandb_api\")\n",
    "    wandb.login(key=api_key)\n",
    "    anony = None\n",
    "except:\n",
    "    anony = \"must\"\n",
    "    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5ed68",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install miditoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ecc165",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24340049",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inference from checkpoint_epoch_50.bin\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# --- Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = \"/kaggle/working/saved/checkpoint_epoch_50.bin\"\n",
    "\n",
    "# --- Load REMI Tokenizer ---\n",
    "with open(\"/kaggle/working/artifacts/vocab_remi.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# --- Load Model ---\n",
    "model = DiscreteDiffusionModel(\n",
    "    vocab_size=len(tokenizer),\n",
    "    d_model=CONFIG['model']['text2midi_model']['decoder_d_model'],\n",
    "    nhead=CONFIG['model']['text2midi_model']['decoder_num_heads'],\n",
    "    num_layers=CONFIG['model']['text2midi_model']['decoder_num_layers'],\n",
    "    dim_feedforward=CONFIG['model']['text2midi_model']['decoder_intermediate_size'],\n",
    "    num_steps=CONFIG['model']['text2midi_model']['num_diffusion_steps'],\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# --- Load T5 Tokenizer ---\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# --- Generate ---\n",
    "caption = \"A chaotic song \"\n",
    "\n",
    "midi_path = generate_midi(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    t5_tokenizer=t5_tokenizer,\n",
    "    caption=caption,\n",
    "    seq_len=CONFIG['model']['text2midi_model']['decoder_max_sequence_length'],\n",
    "    output_dir=CONFIG['training']['text2midi_model']['output_dir'],\n",
    "    device=device,\n",
    "    ddim=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d846b612",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "////////////// edition 80-10-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2881f96",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "//////////used config and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeffa181",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODE=\"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663560b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 12: Execute Mode\n",
    "if MODE == \"train\":\n",
    "    train_model_accelerate(CONFIG)\n",
    "elif MODE == \"build_vocab\":\n",
    "    build_vocab(CONFIG)\n",
    "elif MODE == \"build_vocab_remi\":\n",
    "    build_vocab_remi(CONFIG)\n",
    "else:\n",
    "    raise ValueError(f\"Invalid mode: {MODE}. Choose 'train', 'build_vocab', or 'build_vocab_remi'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098d754",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "import os\n",
    "import pickle\n",
    "import jsonlines\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from transformers import T5Tokenizer, T5EncoderModel, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from accelerate.logging import get_logger\n",
    "from spacy.lang.en import English\n",
    "import wandb\n",
    "import logging\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "\n",
    "# Initialize logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Initialize W&B with API key\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"wandb_api\")\n",
    "    wandb.login(key=api_key)\n",
    "except:\n",
    "    print('W&B login failed. Logging locally. Provide W&B API key in Kaggle Secrets as \"wandb_api\".')\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"model\": {\n",
    "        \"text2midi_model\": {\n",
    "            \"decoder_d_model\": 768,\n",
    "            \"decoder_num_heads\": 12,\n",
    "            \"decoder_num_layers\": 8,\n",
    "            \"decoder_intermediate_size\": 2048,\n",
    "            \"decoder_max_sequence_length\": 1024,\n",
    "            \"num_diffusion_steps\": 1000,\n",
    "            \"ddim_steps\": 100\n",
    "        }\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"text2midi_model\": {\n",
    "            \"learning_rate\": 0.0002,  # Reduced from 0.0005\n",
    "            \"epochs\": 50,\n",
    "            \"max_train_steps\": None,\n",
    "            \"num_warmup_steps\": 2000,\n",
    "            \"gradient_accumulation_steps\": 2,\n",
    "            \"per_device_train_batch_size\": 8,  # Reduced from 16\n",
    "            \"output_dir\": \"/kaggle/working/saved/\",\n",
    "            \"with_tracking\": True,\n",
    "            \"report_to\": \"wandb\",\n",
    "            \"checkpointing_steps\": \"epoch\",\n",
    "            \"save_every\": 5,\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"split_type\": \"80_10_10\",\n",
    "            \"max_grad_norm\": 1.0,  # Added explicit gradient clipping\n",
    "            \"weight_decay\": 0.01,  # Added weight decay for regularization\n",
    "            \"debug_nan\": True,  # Add flag to enable NaN debugging\n",
    "            \"fp16_precision\": True  # Explicitly define precision\n",
    "        }\n",
    "    },\n",
    "    \"raw_data\": {\n",
    "        \"caption_dataset_path\": \"/kaggle/working/captions.json\",\n",
    "        \"dataset_folder\": \"/kaggle/working/output_directory\"\n",
    "    },\n",
    "    \"artifact_folder\": \"/kaggle/working/artifacts\"\n",
    "}\n",
    "\n",
    "# Constants\n",
    "INSTRUMENTS = ['piano', 'chromatic', 'organ', 'guitar', 'bass', 'strings', 'ensemble',\n",
    "               'brass', 'reed', 'pipe', 'synth_lead', 'synth_pad', 'synth_effect',\n",
    "               'ethnic', 'percussive', 'sfx', 'drum']\n",
    "\n",
    "# Verify dataset paths\n",
    "if not os.path.exists(CONFIG['raw_data']['caption_dataset_path']):\n",
    "    logger.error(f\"Caption file not found: {CONFIG['raw_data']['caption_dataset_path']}\")\n",
    "if not os.path.exists(CONFIG['raw_data']['dataset_folder']):\n",
    "    logger.error(f\"Dataset folder not found: {CONFIG['raw_data']['dataset_folder']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26342f43",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "/////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3cfa91",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import jsonlines\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import random\n",
    "import datetime\n",
    "import math\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "def train_model_accelerate(configs):\n",
    "    start_time = time.time()\n",
    "    print(f\"{datetime.datetime.now()}: Starting train_model_accelerate\")\n",
    "\n",
    "    # Initialize Accelerator\n",
    "    print(f\"{datetime.datetime.now()}: Initializing Accelerator\")\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=configs['training']['text2midi_model']['gradient_accumulation_steps'],\n",
    "        mixed_precision='fp16' if configs['training']['text2midi_model']['fp16_precision'] else 'no',\n",
    "        kwargs_handlers=[DistributedDataParallelKwargs(find_unused_parameters=True)],\n",
    "    )\n",
    "\n",
    "    # Set up directories\n",
    "    if accelerator.is_main_process:\n",
    "        output_dir = configs['training']['text2midi_model']['output_dir']\n",
    "        outputs_dir = os.path.join(output_dir, \"outputs\")\n",
    "        os.makedirs(outputs_dir, exist_ok=True)\n",
    "        if configs['training']['text2midi_model']['with_tracking']:\n",
    "            try:\n",
    "                wandb.init(project=\"Text-2-Midi\", settings=wandb.Settings(init_timeout=120))\n",
    "            except Exception as e:\n",
    "                print(f\"{datetime.datetime.now()}: W&B initialization failed: {str(e)}\")\n",
    "                configs['training']['text2midi_model']['with_tracking'] = False\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # Load vocabulary\n",
    "    print(f\"{datetime.datetime.now()}: Loading vocabulary\")\n",
    "    vocab_path = os.path.join(configs['artifact_folder'], \"vocab_remi.pkl\")\n",
    "    if not os.path.exists(vocab_path):\n",
    "        print(f\"{datetime.datetime.now()}: ERROR: Vocabulary file not found: {vocab_path}\")\n",
    "        raise FileNotFoundError(f\"Vocabulary file not found: {vocab_path}\")\n",
    "    with open(vocab_path, \"rb\") as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "\n",
    "    # Load captions\n",
    "    print(f\"{datetime.datetime.now()}: Loading captions\")\n",
    "    caption_path = configs['raw_data']['caption_dataset_path']\n",
    "    if not os.path.exists(caption_path):\n",
    "        print(f\"{datetime.datetime.now()}: ERROR: Caption file not found: {caption_path}\")\n",
    "        raise FileNotFoundError(f\"Caption file not found: {caption_path}\")\n",
    "    caption_start_time = time.time()\n",
    "    with jsonlines.open(caption_path) as reader:\n",
    "        captions = list(reader)\n",
    "    # Limit dataset size for debugging\n",
    "    captions = captions[:10000]  # Use only 10,000 captions\n",
    "    print(f\"{datetime.datetime.now()}: Loaded {len(captions)} captions in {time.time() - caption_start_time:.2f} seconds\")\n",
    "    print(f\"{datetime.datetime.now()}: WARNING: Skipped MIDI file validation. Missing or corrupted files may cause errors.\")\n",
    "\n",
    "    # Split dataset\n",
    "    print(f\"{datetime.datetime.now()}: Splitting dataset\")\n",
    "    train_captions, temp_captions = train_test_split(captions, test_size=0.2, random_state=42)\n",
    "    val_captions, test_captions = train_test_split(temp_captions, test_size=0.5, random_state=42)\n",
    "    print(f\"{datetime.datetime.now()}: Train: {len(train_captions)}, Validation: {len(val_captions)}, Test: {len(test_captions)}\")\n",
    "\n",
    "    # Initialize datasets and dataloaders\n",
    "    print(f\"{datetime.datetime.now()}: Initializing datasets\")\n",
    "    train_dataset = Text2MusicDataset(configs, train_captions, remi_tokenizer=tokenizer, mode=\"train\", shuffle=True)\n",
    "    val_dataset = Text2MusicDataset(configs, val_captions, remi_tokenizer=tokenizer, mode=\"val\")\n",
    "    test_dataset = Text2MusicDataset(configs, test_captions, remi_tokenizer=tokenizer, mode=\"test\")\n",
    "\n",
    "    print(f\"{datetime.datetime.now()}: Initializing dataloaders\")\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=configs['training']['text2midi_model']['per_device_train_batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=configs['training']['text2midi_model']['per_device_train_batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=False\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=configs['training']['text2midi_model']['per_device_train_batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    print(f\"{datetime.datetime.now()}: Initializing model\")\n",
    "    model = DiscreteDiffusionModel(\n",
    "        vocab_size=len(tokenizer),\n",
    "        d_model=configs['model']['text2midi_model']['decoder_d_model'],\n",
    "        nhead=configs['model']['text2midi_model']['decoder_num_heads'],\n",
    "        num_layers=configs['model']['text2midi_model']['decoder_num_layers'],\n",
    "        dim_feedforward=configs['model']['text2midi_model']['decoder_intermediate_size'],\n",
    "        num_steps=configs['model']['text2midi_model']['num_diffusion_steps'],\n",
    "        device=accelerator.device\n",
    "    )\n",
    "\n",
    "    # Load previous best model if it exists\n",
    "    best_model_path = os.path.join(configs['training']['text2midi_model']['output_dir'], 'best_model.bin')\n",
    "    if os.path.exists(best_model_path):\n",
    "        try:\n",
    "            state_dict = torch.load(best_model_path, map_location=accelerator.device)\n",
    "            model.load_state_dict(state_dict)\n",
    "            print(f\"{datetime.datetime.now()}: Loaded checkpoint from {best_model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{datetime.datetime.now()}: Error loading checkpoint {best_model_path}: {str(e)}\")\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{datetime.datetime.now()}: Total number of trainable parameters: {total_params}\")\n",
    "\n",
    "    # Setup optimizer and scheduler\n",
    "    print(f\"{datetime.datetime.now()}: Setting up optimizer and scheduler\")\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=configs['training']['text2midi_model']['learning_rate'] * 0.1,\n",
    "        weight_decay=configs['training']['text2midi_model']['weight_decay']\n",
    "    )\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / configs['training']['text2midi_model']['gradient_accumulation_steps'])\n",
    "    max_train_steps = configs['training']['text2midi_model']['epochs'] * num_update_steps_per_epoch\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=configs['training']['text2midi_model']['lr_scheduler_type'],\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=configs['training']['text2midi_model']['num_warmup_steps'],\n",
    "        num_training_steps=max_train_steps,\n",
    "    )\n",
    "    print(f\"{datetime.datetime.now()}: Training for {configs['training']['text2midi_model']['epochs']} epochs, {max_train_steps} steps\")\n",
    "\n",
    "    # Prepare for distributed training\n",
    "    print(f\"{datetime.datetime.now()}: Preparing for distributed training\")\n",
    "    model, optimizer, lr_scheduler, train_dataloader, val_dataloader, test_dataloader = accelerator.prepare(\n",
    "        model, optimizer, lr_scheduler, train_dataloader, val_dataloader, test_dataloader\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    progress_bar = tqdm(range(max_train_steps), desc=\"Training\", disable=not accelerator.is_local_main_process)\n",
    "    completed_steps = 0\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    no_improve = 0\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    smoother = SmoothingFunction().method1\n",
    "\n",
    "    for epoch in range(configs['training']['text2midi_model']['epochs']):\n",
    "        print(f\"{datetime.datetime.now()}: Starting epoch {epoch + 1}\")\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_bleu = 0\n",
    "        total_train_pitch_sim = 0\n",
    "        total_train_rhythmic_sim = 0\n",
    "        total_train_perplexity = 0\n",
    "        total_train_accuracy = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            try:\n",
    "                encoder_input, attention_mask, x_t, t, x_0 = batch\n",
    "                logits = model(x_t, t, encoder_input, attention_mask)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), x_0.view(-1))\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"{datetime.datetime.now()}: WARNING: Invalid loss (nan/inf) at step {step}, skipping\")\n",
    "                    continue\n",
    "                total_train_loss += loss.detach().float()\n",
    "\n",
    "                # Compute metrics\n",
    "                preds = torch.argmax(logits, dim=-1)  # Predicted tokens\n",
    "                # BLEU score\n",
    "                bleu_scores = []\n",
    "                for pred, ref in zip(preds.cpu().tolist(), x_0.cpu().tolist()):\n",
    "                    bleu_scores.append(sentence_bleu([ref], pred, smoothing_function=smoother))\n",
    "                batch_bleu = np.mean(bleu_scores)\n",
    "                total_train_bleu += batch_bleu\n",
    "\n",
    "                # Pitch similarity (REMI Pitch tokens, MIDI 21–108)\n",
    "                pitch_mask = (x_0 >= 21) & (x_0 <= 108)\n",
    "                if pitch_mask.sum() > 0:\n",
    "                    pitch_sim = (preds[pitch_mask] == x_0[pitch_mask]).float().mean().item()\n",
    "                    total_train_pitch_sim += pitch_sim\n",
    "                else:\n",
    "                    total_train_pitch_sim += 0\n",
    "\n",
    "                # Rhythmic similarity (REMI Duration tokens, placeholder range)\n",
    "                rhythm_mask = (x_0 >= 109) & (x_0 <= 200)  # Adjust based on REMI tokenizer\n",
    "                if rhythm_mask.sum() > 0:\n",
    "                    rhythmic_sim = (preds[rhythm_mask] == x_0[rhythm_mask]).float().mean().item()\n",
    "                    total_train_rhythmic_sim += rhythmic_sim\n",
    "                else:\n",
    "                    total_train_rhythmic_sim += 0\n",
    "\n",
    "                # Perplexity\n",
    "                perplexity = torch.exp(loss.detach()).item()\n",
    "                total_train_perplexity += perplexity if not math.isinf(perplexity) else 0\n",
    "\n",
    "                # Token accuracy\n",
    "                accuracy = (preds == x_0).float().mean().item()\n",
    "                total_train_accuracy += accuracy\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(model.parameters(), configs['training']['text2midi_model']['max_grad_norm'])\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                num_batches += 1\n",
    "            except Exception as e:\n",
    "                print(f\"{datetime.datetime.now()}: ERROR in training step {step}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.set_postfix({\"Loss\": loss.item(), \"BLEU\": batch_bleu})\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "            if completed_steps >= max_train_steps:\n",
    "                break\n",
    "\n",
    "        # Average metrics over batches\n",
    "        avg_loss = total_train_loss / num_batches if num_batches > 0 else float('inf')\n",
    "        avg_bleu = total_train_bleu / num_batches if num_batches > 0 else 0\n",
    "        avg_pitch_sim = total_train_pitch_sim / num_batches if num_batches > 0 else 0\n",
    "        avg_rhythmic_sim = total_train_rhythmic_sim / num_batches if num_batches > 0 else 0\n",
    "        avg_perplexity = total_train_perplexity / num_batches if num_batches > 0 else float('inf')\n",
    "        avg_accuracy = total_train_accuracy / num_batches if num_batches > 0 else 0\n",
    "\n",
    "        # Log metrics to W&B\n",
    "        if configs['training']['text2midi_model']['with_tracking'] and accelerator.is_main_process:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": avg_loss,\n",
    "                \"train_bleu\": avg_bleu,\n",
    "                \"train_pitch_sim\": avg_pitch_sim,\n",
    "                \"train_rhythmic_sim\": avg_rhythmic_sim,\n",
    "                \"train_perplexity\": avg_perplexity,\n",
    "                \"train_accuracy\": avg_accuracy\n",
    "            })\n",
    "\n",
    "        print(f\"{datetime.datetime.now()}: Epoch {epoch + 1} completed - Loss: {avg_loss:.4f}, BLEU: {avg_bleu:.4f}, \"\n",
    "              f\"Pitch Sim: {avg_pitch_sim:.4f}, Rhythmic Sim: {avg_rhythmic_sim:.4f}, Perplexity: {avg_perplexity:.4f}, \"\n",
    "              f\"Accuracy: {avg_accuracy:.4f}\")\n",
    "                # Save checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0 and accelerator.is_main_process:\n",
    "            checkpoint_path = os.path.join(\n",
    "                configs['training']['text2midi_model']['output_dir'],\n",
    "                f\"checkpoint_epoch_{epoch + 1}.bin\"\n",
    "            )\n",
    "            torch.save(accelerator.unwrap_model(model).state_dict(), checkpoint_path)\n",
    "            print(f\"{datetime.datetime.now()}: Saved checkpoint at {checkpoint_path}\")\n",
    "\n",
    "\n",
    "    print(f\"{datetime.datetime.now()}: Training completed in {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bc9e5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "//////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e6430b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "/// USED CODE IS FOR TRAINING ? EVERYTHING BEFORE THIS REALTED TO TRAINING IS UNUSED //VOCAB CREATION AND INFERANCE IS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2c229",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import jsonlines\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from accelerate import Accelerator\n",
    "from transformers import T5Tokenizer, T5EncoderModel, get_scheduler\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize W&B login once at the start\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"wandb_api\")\n",
    "    wandb.login(key=api_key)\n",
    "    logger.info(\"W&B login successful\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"W&B login failed: {e}. Logging locally.\")\n",
    "    WANDB_ENABLED = False\n",
    "else:\n",
    "    WANDB_ENABLED = True\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"model\": {\n",
    "        \"text2midi_model\": {\n",
    "            \"decoder_d_model\": 768,\n",
    "            \"decoder_num_heads\": 12,\n",
    "            \"decoder_num_layers\": 8,\n",
    "            \"decoder_intermediate_size\": 2048,\n",
    "            \"decoder_max_sequence_length\": 1024,\n",
    "            \"num_diffusion_steps\": 1000,\n",
    "            \"ddim_steps\": 100\n",
    "        }\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"text2midi_model\": {\n",
    "            \"learning_rate\": 0.0002,\n",
    "            \"epochs\": 50,\n",
    "            \"num_warmup_steps\": 2000,\n",
    "            \"gradient_accumulation_steps\": 2,\n",
    "            \"per_device_train_batch_size\": 8,\n",
    "            \"output_dir\": \"/kaggle/working/saved/\",\n",
    "            \"with_tracking\": WANDB_ENABLED,\n",
    "            \"checkpointing_steps\": \"epoch\",\n",
    "            \"save_every\": 5,\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"max_grad_norm\": 1.0,\n",
    "            \"weight_decay\": 0.01,\n",
    "            \"fp16_precision\": True\n",
    "        }\n",
    "    },\n",
    "    \"raw_data\": {\n",
    "        \"caption_dataset_path\": \"/kaggle/working/captions.json\",\n",
    "        \"dataset_folder\": \"/kaggle/working/output_directory\"\n",
    "    },\n",
    "    \"artifact_folder\": \"/kaggle/working/artifacts\"\n",
    "}\n",
    "\n",
    "# Helper Functions\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = torch.arange(timesteps, dtype=torch.float32)\n",
    "    f_t = torch.cos(((steps / timesteps + s) / (1.0 + s) * math.pi / 2)) ** 2\n",
    "    betas = 1.0 - f_t / torch.roll(f_t, shifts=1, dims=0)\n",
    "    betas = torch.clamp(betas, 0.0, 0.999)\n",
    "    betas[0] = 0.0001\n",
    "    return betas\n",
    "\n",
    "# Model Components\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class DiscreteDiffusionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=768, nhead=12, num_layers=8, dim_feedforward=2048,\n",
    "                 num_steps=1000, dropout=0.1, device=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.num_steps = num_steps\n",
    "        self.device = device\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.time_emb = nn.Embedding(num_steps, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=5000).to(device)\n",
    "\n",
    "        # FLAN-T5 encoder\n",
    "        self.encoder = T5EncoderModel.from_pretrained(\"google/flan-t5-base\").to(device)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Project FLAN-T5 output (768) to d_model\n",
    "        self.text_projection = nn.Linear(768, d_model).to(device)\n",
    "\n",
    "        # Transformer-based denoiser\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\"\n",
    "        )\n",
    "        self.denoiser = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output layer\n",
    "        self.projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # D3PM: Uniform transition matrix\n",
    "        self.Q = torch.ones(vocab_size, vocab_size, device=device) / vocab_size\n",
    "        self.Q_bar = torch.ones(num_steps, vocab_size, vocab_size, device=device)\n",
    "        self.log_Q = torch.log(self.Q + 1e-10)\n",
    "\n",
    "        # Noise schedule (cosine)\n",
    "        self.betas = cosine_beta_schedule(num_steps).to(device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bar = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "        # Precompute Q_bar for each timestep\n",
    "        for t in range(num_steps):\n",
    "            alpha_bar_t = self.alpha_bar[t]\n",
    "            self.Q_bar[t] = alpha_bar_t * torch.eye(vocab_size, device=device) + \\\n",
    "                           (1 - alpha_bar_t) * self.Q\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def forward(self, x_t, t, src, src_mask):\n",
    "        x_emb = self.token_emb(x_t) * math.sqrt(self.d_model)\n",
    "        x_emb = self.pos_encoder(x_emb.transpose(0, 1)).transpose(0, 1)\n",
    "        t_emb = self.time_emb(t).unsqueeze(1)\n",
    "        x_emb = x_emb + t_emb\n",
    "        memory = self.encoder(src, attention_mask=src_mask).last_hidden_state\n",
    "        memory = self.text_projection(memory)\n",
    "        output = self.denoiser(x_emb, memory, memory_mask=None)\n",
    "        logits = self.projection(output)\n",
    "        return logits\n",
    "\n",
    "    def sample(self, src, src_mask, seq_len, num_steps=None, ddim=False, eta=0.0):\n",
    "        device = src.device\n",
    "        batch_size = src.size(0)\n",
    "        x_t = torch.randint(0, self.vocab_size, (batch_size, seq_len), device=device)\n",
    "        num_steps = num_steps or self.num_steps\n",
    "\n",
    "        if ddim:\n",
    "            step_indices = torch.linspace(0, self.num_steps - 1, steps=self.num_steps // num_steps + 1, device=device).long()\n",
    "        else:\n",
    "            step_indices = torch.arange(num_steps, device=device)\n",
    "\n",
    "        for i in reversed(range(len(step_indices))):\n",
    "            t = step_indices[i]\n",
    "            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                logits = self(x_t, t_tensor, src, src_mask)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                x_t_new = torch.multinomial(probs.view(-1, self.vocab_size), num_samples=1).view(batch_size, seq_len)\n",
    "\n",
    "            if ddim and i > 0:\n",
    "                t_prev = step_indices[i - 1]\n",
    "                alpha_bar_t = self.alpha_bar[t]\n",
    "                alpha_bar_t_prev = self.alpha_bar[t_prev]\n",
    "                sigma = eta * torch.sqrt((1 - alpha_bar_t_prev) / (1 - alpha_bar_t) * (1 - alpha_bar_t / alpha_bar_t_prev))\n",
    "                x_t = torch.where(\n",
    "                    torch.rand_like(x_t.float()) < sigma,\n",
    "                    torch.randint(0, self.vocab_size, x_t.shape, device=device),\n",
    "                    x_t_new\n",
    "                )\n",
    "            else:\n",
    "                x_t = x_t_new\n",
    "\n",
    "        return x_t\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "# Dataset\n",
    "class Text2MusicDataset(Dataset):\n",
    "    def __init__(self, configs, captions, remi_tokenizer, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        self.captions = captions\n",
    "        self.dataset_path = configs[\"raw_data\"][\"dataset_folder\"]\n",
    "        self.remi_tokenizer = remi_tokenizer\n",
    "        self.t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        self.decoder_max_sequence_length = configs[\"model\"][\"text2midi_model\"][\"decoder_max_sequence_length\"]\n",
    "        self.num_steps = configs[\"model\"][\"text2midi_model\"][\"num_diffusion_steps\"]\n",
    "        self.vocab_size = len(remi_tokenizer)\n",
    "\n",
    "        # Noise schedule\n",
    "        self.betas = cosine_beta_schedule(self.num_steps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bar = torch.cumprod(self.alphas, dim=0)\n",
    "        self.Q = torch.ones(self.vocab_size, self.vocab_size) / self.vocab_size\n",
    "\n",
    "        # Validate MIDI files\n",
    "        valid_captions = []\n",
    "        for cap in captions:\n",
    "            if not isinstance(cap, dict) or \"caption\" not in cap or \"location\" not in cap:\n",
    "                logger.warning(f\"Invalid caption format: {cap}\")\n",
    "                continue\n",
    "            midi_path = os.path.join(self.dataset_path, cap[\"location\"])\n",
    "            if os.path.exists(midi_path):\n",
    "                valid_captions.append(cap)\n",
    "            else:\n",
    "                logger.warning(f\"MIDI file not found: {midi_path}\")\n",
    "        self.captions = valid_captions\n",
    "        if not self.captions:\n",
    "            raise ValueError(\"No valid captions found after validation\")\n",
    "        logger.info(f\"Dataset size after validation: {len(self.captions)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.captions[idx][\"caption\"]\n",
    "        midi_filepath = os.path.join(self.dataset_path, self.captions[idx][\"location\"])\n",
    "\n",
    "        try:\n",
    "            tokens = self.remi_tokenizer(midi_filepath)\n",
    "            tokenized_midi = ([self.remi_tokenizer[\"BOS_None\"]] + tokens.ids +\n",
    "                             [self.remi_tokenizer[\"EOS_None\"]]) if tokens.ids else \\\n",
    "                            [self.remi_tokenizer[\"BOS_None\"], self.remi_tokenizer[\"EOS_None\"]]\n",
    "            tokenized_midi = torch.tensor(tokenized_midi)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error tokenizing MIDI file {midi_filepath}: {str(e)}\")\n",
    "            tokenized_midi = torch.tensor([self.remi_tokenizer[\"BOS_None\"], self.remi_tokenizer[\"EOS_None\"]])\n",
    "\n",
    "        if len(tokenized_midi) < self.decoder_max_sequence_length:\n",
    "            x_0 = F.pad(tokenized_midi, (0, self.decoder_max_sequence_length - len(tokenized_midi))).long()\n",
    "        else:\n",
    "            x_0 = tokenized_midi[:self.decoder_max_sequence_length].long()\n",
    "\n",
    "        t = torch.randint(0, self.num_steps, (1,)).item()\n",
    "        alpha_bar_t = self.alpha_bar[t]\n",
    "        Q_t = alpha_bar_t * torch.eye(self.vocab_size) + (1 - alpha_bar_t) * self.Q\n",
    "        probs = F.one_hot(x_0, num_classes=self.vocab_size).float() @ Q_t\n",
    "        x_t = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "        inputs = self.t5_tokenizer(caption, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        return (inputs[\"input_ids\"].squeeze(0), inputs[\"attention_mask\"].squeeze(0),\n",
    "                x_t.long(), t, x_0.long())\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = nn.utils.rnn.pad_sequence([item[0] for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = nn.utils.rnn.pad_sequence([item[1] for item in batch], batch_first=True, padding_value=0)\n",
    "    x_t = nn.utils.rnn.pad_sequence([item[2] for item in batch], batch_first=True, padding_value=0)\n",
    "    t = torch.tensor([item[3] for item in batch])\n",
    "    x_0 = nn.utils.rnn.pad_sequence([item[4] for item in batch], batch_first=True, padding_value=0)\n",
    "    return input_ids, attention_mask, x_t, t, x_0\n",
    "\n",
    "# Training Function\n",
    "def train_model(configs):\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=configs[\"training\"][\"text2midi_model\"][\"gradient_accumulation_steps\"],\n",
    "        mixed_precision=\"fp16\" if configs[\"training\"][\"text2midi_model\"][\"fp16_precision\"] else \"no\"\n",
    "    )\n",
    "\n",
    "    # Set up directories\n",
    "    if accelerator.is_main_process:\n",
    "        output_dir = configs[\"training\"][\"text2midi_model\"][\"output_dir\"]\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        if configs[\"training\"][\"text2midi_model\"][\"with_tracking\"]:\n",
    "            wandb.init(project=\"Text-2-Midi\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    vocab_path = os.path.join(configs[\"artifact_folder\"], \"vocab_remi.pkl\")\n",
    "    if not os.path.exists(vocab_path):\n",
    "        raise FileNotFoundError(f\"Vocabulary file not found: {vocab_path}\")\n",
    "    with open(vocab_path, \"rb\") as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "\n",
    "    # Load captions\n",
    "    caption_path = configs[\"raw_data\"][\"caption_dataset_path\"]\n",
    "    if not os.path.exists(caption_path):\n",
    "        raise FileNotFoundError(f\"Caption file not found: {caption_path}\")\n",
    "    with jsonlines.open(caption_path) as reader:\n",
    "        captions = list(reader)[:10000]  # Limit to 10,000 captions\n",
    "    if not captions:\n",
    "        raise ValueError(\"No captions loaded from file\")\n",
    "    train_captions, temp_captions = train_test_split(captions, test_size=0.2, random_state=42)\n",
    "    val_captions, test_captions = train_test_split(temp_captions, test_size=0.5, random_state=42)\n",
    "    logger.info(f\"Train: {len(train_captions)}, Val: {len(val_captions)}, Test: {len(test_captions)}\")\n",
    "\n",
    "    # Initialize datasets and dataloaders\n",
    "    train_dataset = Text2MusicDataset(configs, train_captions, remi_tokenizer=tokenizer, mode=\"train\")\n",
    "    val_dataset = Text2MusicDataset(configs, val_captions, remi_tokenizer=tokenizer, mode=\"val\")\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=configs[\"training\"][\"text2midi_model\"][\"per_device_train_batch_size\"],\n",
    "        shuffle=True, num_workers=4, collate_fn=collate_fn, drop_last=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=configs[\"training\"][\"text2midi_model\"][\"per_device_train_batch_size\"],\n",
    "        shuffle=False, num_workers=4, collate_fn=collate_fn, drop_last=False\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = DiscreteDiffusionModel(\n",
    "        vocab_size=len(tokenizer),\n",
    "        d_model=configs[\"model\"][\"text2midi_model\"][\"decoder_d_model\"],\n",
    "        nhead=configs[\"model\"][\"text2midi_model\"][\"decoder_num_heads\"],\n",
    "        num_layers=configs[\"model\"][\"text2midi_model\"][\"decoder_num_layers\"],\n",
    "        dim_feedforward=configs[\"model\"][\"text2midi_model\"][\"decoder_intermediate_size\"],\n",
    "        num_steps=configs[\"model\"][\"text2midi_model\"][\"num_diffusion_steps\"],\n",
    "        device=accelerator.device\n",
    "    )\n",
    "\n",
    "    # Load checkpoint if available\n",
    "    best_model_path = os.path.join(output_dir, \"best_model.bin\")\n",
    "    if os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=accelerator.device))\n",
    "        logger.info(f\"Loaded checkpoint from {best_model_path}\")\n",
    "\n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=configs[\"training\"][\"text2midi_model\"][\"learning_rate\"],\n",
    "        weight_decay=configs[\"training\"][\"text2midi_model\"][\"weight_decay\"]\n",
    "    )\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / configs[\"training\"][\"text2midi_model\"][\"gradient_accumulation_steps\"])\n",
    "    max_train_steps = configs[\"training\"][\"text2midi_model\"][\"epochs\"] * num_update_steps_per_epoch\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=configs[\"training\"][\"text2midi_model\"][\"lr_scheduler_type\"],\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=configs[\"training\"][\"text2midi_model\"][\"num_warmup_steps\"],\n",
    "        num_training_steps=max_train_steps\n",
    "    )\n",
    "\n",
    "    # Prepare for distributed training\n",
    "    model, optimizer, lr_scheduler, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "        model, optimizer, lr_scheduler, train_dataloader, val_dataloader\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    progress_bar = tqdm(range(max_train_steps), desc=\"Training\", disable=not accelerator.is_local_main_process)\n",
    "    completed_steps = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(configs[\"training\"][\"text2midi_model\"][\"epochs\"]):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_accuracy = 0\n",
    "        total_train_perplexity = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            with accelerator.accumulate(model):\n",
    "                encoder_input, attention_mask, x_t, t, x_0 = batch\n",
    "                logits = model(x_t, t, encoder_input, attention_mask)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), x_0.view(-1))\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    logger.warning(f\"Invalid loss at epoch {epoch+1}, skipping\")\n",
    "                    continue\n",
    "\n",
    "                total_train_loss += loss.detach().float()\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                total_train_accuracy += (preds == x_0).float().mean().item()\n",
    "                total_train_perplexity += torch.exp(loss.detach()).item()\n",
    "                num_batches += 1\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), configs[\"training\"][\"text2midi_model\"][\"max_grad_norm\"])\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "        if num_batches == 0:\n",
    "            logger.warning(f\"No valid batches processed in epoch {epoch+1}\")\n",
    "            continue\n",
    "\n",
    "        avg_loss = total_train_loss / num_batches\n",
    "        avg_accuracy = total_train_accuracy / num_batches\n",
    "        avg_perplexity = total_train_perplexity / num_batches\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                encoder_input, attention_mask, x_t, t, x_0 = batch\n",
    "                logits = model(x_t, t, encoder_input, attention_mask)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), x_0.view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        avg_val_loss = total_val_loss / val_batches if val_batches > 0 else float(\"inf\")\n",
    "\n",
    "        # Log metrics\n",
    "        if configs[\"training\"][\"text2midi_model\"][\"with_tracking\"] and accelerator.is_main_process:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": avg_loss,\n",
    "                \"train_accuracy\": avg_accuracy,\n",
    "                \"train_perplexity\": avg_perplexity,\n",
    "                \"val_loss\": avg_val_loss\n",
    "            })\n",
    "\n",
    "        logger.info(f\"Epoch {epoch+1}: Train Loss={avg_loss:.4f}, Accuracy={avg_accuracy:.4f}, \"\n",
    "                    f\"Perplexity={avg_perplexity:.4f}, Val Loss={avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % configs[\"training\"][\"text2midi_model\"][\"save_every\"] == 0 and accelerator.is_main_process:\n",
    "            checkpoint_path = os.path.join(output_dir, f\"checkpoint_epoch_{epoch+1}.bin\")\n",
    "            torch.save(accelerator.unwrap_model(model).state_dict(), checkpoint_path)\n",
    "            logger.info(f\"Saved checkpoint at {checkpoint_path}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss and accelerator.is_main_process:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(accelerator.unwrap_model(model).state_dict(), best_model_path)\n",
    "            logger.info(f\"Saved best model at {best_model_path}\")\n",
    "\n",
    "    accelerator.end_training()\n",
    "\n",
    "# Inference Function\n",
    "\n",
    "# Vocabulary Building\n",
    "def build_vocab_remi(configs):\n",
    "    TOKENIZER_PARAMS = {\n",
    "        \"pitch_range\": (21, 109),\n",
    "        \"beat_res\": {(0, 1): 12, (1, 2): 4, (2, 4): 2, (4, 8): 1},\n",
    "        \"num_velocities\": 32,\n",
    "        \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n",
    "        \"use_chords\": False,\n",
    "        \"use_rests\": False,\n",
    "        \"use_tempos\": True,\n",
    "        \"use_time_signatures\": True,\n",
    "        \"use_programs\": True,\n",
    "        \"num_tempos\": 32,\n",
    "        \"tempo_range\": (40, 250),\n",
    "    }\n",
    "    config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "    tokenizer = REMI(config)\n",
    "    logger.info(f\"Vocabulary length: {tokenizer.vocab_size}\")\n",
    "    vocab_path = os.path.join(configs[\"artifact_folder\"], \"vocab_remi.pkl\")\n",
    "    os.makedirs(configs[\"artifact_folder\"], exist_ok=True)\n",
    "    with open(vocab_path, \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    logger.info(f\"Vocabulary saved to {vocab_path}\")\n",
    "\n",
    "# Main Execution\n",
    "\n",
    "def generate_midi(model, tokenizer, t5_tokenizer, caption, seq_len, output_dir, device, ddim=True, num_steps=100):\n",
    "    model.eval()\n",
    "    inputs = t5_tokenizer(caption, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokens = model.sample(input_ids, attention_mask, seq_len, num_steps=num_steps, ddim=ddim)\n",
    "\n",
    "    token_ids = tokens[0].cpu().numpy().tolist()\n",
    "    if token_ids[0] == tokenizer[\"BOS_None\"]:\n",
    "        token_ids = token_ids[1:]\n",
    "    if token_ids[-1] == tokenizer[\"EOS_None\"]:\n",
    "        token_ids = token_ids[:-1]\n",
    "\n",
    "    try:\n",
    "        midi_score = tokenizer.decode(token_ids)\n",
    "        output_path = os.path.join(output_dir, f\"generated_{int(time.time())}.mid\")\n",
    "        midi_score.dump_midi(output_path)\n",
    "        logger.info(f\"Generated MIDI saved to {output_path}\")\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting tokens to MIDI: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def main(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        train_model(CONFIG)\n",
    "    elif mode == \"build_vocab_remi\":\n",
    "        build_vocab_remi(CONFIG)\n",
    "    elif mode == \"infer\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        with open(os.path.join(CONFIG[\"artifact_folder\"], \"vocab_remi.pkl\"), \"rb\") as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        model = DiscreteDiffusionModel(\n",
    "            vocab_size=len(tokenizer),\n",
    "            d_model=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_d_model\"],\n",
    "            nhead=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_num_heads\"],\n",
    "            num_layers=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_num_layers\"],\n",
    "            dim_feedforward=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_intermediate_size\"],\n",
    "            num_steps=CONFIG[\"model\"][\"text2midi_model\"][\"num_diffusion_steps\"],\n",
    "            device=device\n",
    "        ).to(device)\n",
    "        checkpoint_path = os.path.join(CONFIG[\"training\"][\"text2midi_model\"][\"output_dir\"], \"best_model.bin\")\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "        t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        caption = \"A melodic electronic composition with classical influences, featuring a string ensemble, trumpet, brass section, synth strings, and drums. Set in F# minor with a 4/4 time signature, it moves at an Allegro tempo. The mood evokes a cinematic, spacious, and epic atmosphere while maintaining a sense of relaxation.\"\n",
    "        generate_long_midi(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            t5_tokenizer=t5_tokenizer,\n",
    "            caption=\"A dramatic orchestral theme with sweeping strings and pounding drums.\",\n",
    "            seq_len=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_max_sequence_length\"],\n",
    "            output_dir=CONFIG[\"training\"][\"text2midi_model\"][\"output_dir\"],\n",
    "            device=device,\n",
    "            num_chunks=20,\n",
    "            remove_silence=True,\n",
    "            num_steps=200,\n",
    "            ddim=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}. Choose 'train', 'build_vocab_remi', or 'infer'\")\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    mode = os.getenv(\"MODE\", \"train\")\n",
    "    main(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24344c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "////genrate with post process :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea943b06",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install miditoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac36ee",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "// old genrate midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7b1e3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_midi(model, tokenizer, t5_tokenizer, caption, seq_len, output_dir, device, ddim=True, num_steps=100):\n",
    "    model.eval()\n",
    "    inputs = t5_tokenizer(caption, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokens = model.sample(input_ids, attention_mask, seq_len, num_steps=num_steps, ddim=ddim)\n",
    "\n",
    "    token_ids = tokens[0].cpu().numpy().tolist()\n",
    "    if token_ids[0] == tokenizer[\"BOS_None\"]:\n",
    "        token_ids = token_ids[1:]\n",
    "    if token_ids[-1] == tokenizer[\"EOS_None\"]:\n",
    "        token_ids = token_ids[:-1]\n",
    "\n",
    "    try:\n",
    "        midi_score = tokenizer.decode(token_ids)\n",
    "        output_path = os.path.join(output_dir, f\"generated_{int(time.time())}.mid\")\n",
    "        midi_score.dump_midi(output_path)\n",
    "        logger.info(f\"Generated MIDI saved to {output_path}\")\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting tokens to MIDI: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ab85e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "////old main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137eddf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(mode=\"infer\"):\n",
    "    if mode == \"train\":\n",
    "        train_model(CONFIG)\n",
    "    elif mode == \"build_vocab_remi\":\n",
    "        build_vocab_remi(CONFIG)\n",
    "    elif mode == \"infer\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        with open(os.path.join(CONFIG[\"artifact_folder\"], \"vocab_remi.pkl\"), \"rb\") as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        model = DiscreteDiffusionModel(\n",
    "            vocab_size=len(tokenizer),\n",
    "            d_model=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_d_model\"],\n",
    "            nhead=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_num_heads\"],\n",
    "            num_layers=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_num_layers\"],\n",
    "            dim_feedforward=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_intermediate_size\"],\n",
    "            num_steps=CONFIG[\"model\"][\"text2midi_model\"][\"num_diffusion_steps\"],\n",
    "            device=device\n",
    "        ).to(device)\n",
    "        checkpoint_path = os.path.join(CONFIG[\"training\"][\"text2midi_model\"][\"output_dir\"], \"best_model.bin\")\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "        t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        caption = \"A melodic electronic composition with classical influences, featuring a string ensemble, trumpet, brass section, synth strings, and drums. Set in F# minor with a 4/4 time signature, it moves at an Allegro tempo. The mood evokes a cinematic, spacious, and epic atmosphere while maintaining a sense of relaxation.\"\n",
    "        generate_long_midi(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            t5_tokenizer=t5_tokenizer,\n",
    "            caption=\"A dramatic orchestral theme with sweeping strings and pounding drums.\",\n",
    "            seq_len=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_max_sequence_length\"],\n",
    "            output_dir=CONFIG[\"training\"][\"text2midi_model\"][\"output_dir\"],\n",
    "            device=device,\n",
    "            num_chunks=20,\n",
    "            remove_silence=True,\n",
    "            num_steps=200,\n",
    "            ddim=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}. Choose 'train', 'build_vocab_remi', or 'infer'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da785f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_midi(\n",
    "            model, tokenizer, t5_tokenizer, caption,\n",
    "            seq_len=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_max_sequence_length\"],\n",
    "            output_dir=CONFIG[\"training\"][\"text2midi_model\"][\"output_dir\"],\n",
    "            device=device, ddim=True, num_steps=200\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee32be3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T15:49:10.641832Z",
     "iopub.status.busy": "2025-05-17T15:49:10.641062Z",
     "iopub.status.idle": "2025-05-17T15:49:16.110224Z",
     "shell.execute_reply": "2025-05-17T15:49:16.109258Z",
     "shell.execute_reply.started": "2025-05-17T15:49:10.641804Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90388a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T15:49:16.112073Z",
     "iopub.status.busy": "2025-05-17T15:49:16.111716Z",
     "iopub.status.idle": "2025-05-17T15:49:20.714665Z",
     "shell.execute_reply": "2025-05-17T15:49:20.713899Z",
     "shell.execute_reply.started": "2025-05-17T15:49:16.112052Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install midi_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c9682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T15:49:20.716059Z",
     "iopub.status.busy": "2025-05-17T15:49:20.715731Z",
     "iopub.status.idle": "2025-05-17T15:49:28.101144Z",
     "shell.execute_reply": "2025-05-17T15:49:28.100183Z",
     "shell.execute_reply.started": "2025-05-17T15:49:20.716016Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install laion_clap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0c6ee",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0355b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T15:51:09.093339Z",
     "iopub.status.busy": "2025-05-17T15:51:09.092053Z",
     "iopub.status.idle": "2025-05-17T15:51:30.077029Z",
     "shell.execute_reply": "2025-05-17T15:51:30.076370Z",
     "shell.execute_reply.started": "2025-05-17T15:51:09.093282Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import jsonlines\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from accelerate import Accelerator\n",
    "from transformers import T5Tokenizer, T5EncoderModel, get_scheduler\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import re\n",
    "import tempfile\n",
    "import music21\n",
    "import pretty_midi\n",
    "\n",
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize W&B login\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"wandb_api\")\n",
    "    wandb.login(key=api_key)\n",
    "    logger.info(\"W&B login successful\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"W&B login failed: {e}. Logging locally.\")\n",
    "    WANDB_ENABLED = False\n",
    "else:\n",
    "    WANDB_ENABLED = True\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"model\": {\n",
    "        \"text2midi_model\": {\n",
    "            \"decoder_d_model\": 1024,\n",
    "            \"decoder_num_heads\": 8,\n",
    "            \"decoder_num_layers\": 12,\n",
    "            \"decoder_intermediate_size\": 2048,\n",
    "            \"decoder_max_sequence_length\": 2048,\n",
    "            \"num_diffusion_steps\": 1000,\n",
    "            \"ddim_steps\": 100\n",
    "        }\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"text2midi_model\": {\n",
    "            \"learning_rate\": 0.0003,\n",
    "            \"epochs\": 50,\n",
    "            \"num_warmup_steps\": 1000,\n",
    "            \"gradient_accumulation_steps\": 2,\n",
    "            \"per_device_train_batch_size\": 8,\n",
    "            \"output_dir\": \"/kaggle/working/saved/\",\n",
    "            \"with_tracking\": WANDB_ENABLED,\n",
    "            \"checkpointing_steps\": \"epoch\",\n",
    "            \"save_every\": 5,\n",
    "            \"lr_scheduler_type\": \"cosine\",\n",
    "            \"max_grad_norm\": 1.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"fp16_precision\": True\n",
    "        }\n",
    "    },\n",
    "    \"raw_data\": {\n",
    "        \"caption_dataset_path\": \"/kaggle/working/captions.json\",\n",
    "        \"dataset_folder\": \"/kaggle/working/output_directory\"\n",
    "    },\n",
    "    \"artifact_folder\": \"/kaggle/working/artifacts\"\n",
    "}\n",
    "\n",
    "# Helper Functions\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = torch.arange(timesteps, dtype=torch.float32)\n",
    "    f_t = torch.cos(((steps / timesteps + s) / (1.0 + s) * math.pi / 2)) ** 2\n",
    "    betas = 1.0 - f_t / torch.roll(f_t, shifts=1, dims=0)\n",
    "    betas = torch.clamp(betas, 0.0, 0.999)\n",
    "    betas[0] = 0.0001\n",
    "    return betas\n",
    "\n",
    "# Evaluation Metric Helpers\n",
    "TEMPO_TERM_TO_BPM = {\n",
    "    \"largo\": 50,\n",
    "    \"adagio\": 70,\n",
    "    \"andante\": 90,\n",
    "    \"moderato\": 110,\n",
    "    \"allegro\": 140,\n",
    "    \"presto\": 180,\n",
    "}\n",
    "\n",
    "def parse_caption(caption):\n",
    "    tempo_match = re.search(r'at an? (\\w+) tempo', caption, re.IGNORECASE)\n",
    "    tempo_term = tempo_match.group(1) if tempo_match else None\n",
    "    key_match = re.search(r'in ([A-G][#b]? (major|minor))', caption, re.IGNORECASE)\n",
    "    key = key_match.group(1) if key_match else None\n",
    "    return tempo_term, key\n",
    "\n",
    "def map_tempo_term_to_bin(tempo_term):\n",
    "    if tempo_term is None:\n",
    "        return None\n",
    "    tempo_term = tempo_term.lower()\n",
    "    if tempo_term in TEMPO_TERM_TO_BPM:\n",
    "        target_bpm = TEMPO_TERM_TO_BPM[tempo_term]\n",
    "        min_tempo = 40\n",
    "        max_tempo = 250\n",
    "        num_tempos = 32\n",
    "        bin_width = (max_tempo - min_tempo) / (num_tempos - 1)\n",
    "        bin_index = round((target_bpm - min_tempo) / bin_width)\n",
    "        return max(0, min(bin_index, num_tempos - 1))\n",
    "    return None\n",
    "\n",
    "def extract_tempo_bin(tokens, tokenizer):\n",
    "    min_tempo = 40\n",
    "    max_tempo = 250\n",
    "    num_tempos = 32\n",
    "    bin_width = (max_tempo - min_tempo) / (num_tempos - 1)\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_str = tokenizer[token]\n",
    "        if token_str.startswith(\"Tempo_\"):\n",
    "            try:\n",
    "                tempo_value = float(token_str.split('_')[1])\n",
    "                bin_index = round((tempo_value - min_tempo) / bin_width)\n",
    "                return max(0, min(bin_index, num_tempos - 1))\n",
    "            except (ValueError, IndexError) as e:\n",
    "                logger.warning(f\"Failed to parse tempo token {token_str}: {e}\")\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def detect_key(midi_path):\n",
    "    try:\n",
    "        score = music21.converter.parse(midi_path)\n",
    "        key = score.analyze('key')\n",
    "        return key.tonic.name + ' ' + key.mode\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Key detection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def compression_ratio(tokens):\n",
    "    unique_tokens = len(set([t for t in tokens if t != 0]))  # Exclude padding\n",
    "    total_tokens = len([t for t in tokens if t != 0])\n",
    "    return unique_tokens / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "# Model Components\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class DiscreteDiffusionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=768, nhead=12, num_layers=8, dim_feedforward=2048,\n",
    "                 num_steps=1000, dropout=0.1, device=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.num_steps = num_steps\n",
    "        self.device = device\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.time_emb = nn.Embedding(num_steps, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=5000).to(device)\n",
    "        self.encoder = T5EncoderModel.from_pretrained(\"google/flan-t5-base\").to(device)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.text_projection = nn.Linear(768, d_model).to(device)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\"\n",
    "        )\n",
    "        self.denoiser = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.Q = torch.ones(vocab_size, vocab_size, device=device) / vocab_size\n",
    "        self.Q_bar = torch.ones(num_steps, vocab_size, vocab_size, device=device)\n",
    "        self.log_Q = torch.log(self.Q + 1e-10)\n",
    "        self.betas = cosine_beta_schedule(num_steps).to(device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bar = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "        for t in range(num_steps):\n",
    "            alpha_bar_t = self.alpha_bar[t]\n",
    "            self.Q_bar[t] = alpha_bar_t * torch.eye(vocab_size, device=device) + \\\n",
    "                           (1 - alpha_bar_t) * self.Q\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def forward(self, x_t, t, src, src_mask):\n",
    "        x_emb = self.token_emb(x_t) * math.sqrt(self.d_model)\n",
    "        x_emb = self.pos_encoder(x_emb.transpose(0, 1)).transpose(0, 1)\n",
    "        t_emb = self.time_emb(t).unsqueeze(1)\n",
    "        x_emb = x_emb + t_emb\n",
    "        memory = self.encoder(src, attention_mask=src_mask).last_hidden_state\n",
    "        memory = self.text_projection(memory)\n",
    "        output = self.denoiser(x_emb, memory, memory_mask=None)\n",
    "        logits = self.projection(output)\n",
    "        return logits\n",
    "\n",
    "    def sample(self, src, src_mask, seq_len, num_steps=None, ddim=False, eta=0.0):\n",
    "        device = src.device\n",
    "        batch_size = src.size(0)\n",
    "        x_t = torch.randint(0, self.vocab_size, (batch_size, seq_len), device=device)\n",
    "        num_steps = num_steps or self.num_steps\n",
    "\n",
    "        if ddim:\n",
    "            step_indices = torch.linspace(0, self.num_steps - 1, steps=self.num_steps // num_steps + 1, device=device).long()\n",
    "        else:\n",
    "            step_indices = torch.arange(num_steps, device=device)\n",
    "\n",
    "        for i in reversed(range(len(step_indices))):\n",
    "            t = step_indices[i]\n",
    "            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                logits = self(x_t, t_tensor, src, src_mask)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                x_t_new = torch.multinomial(probs.view(-1, self.vocab_size), num_samples=1).view(batch_size, seq_len)\n",
    "\n",
    "            if ddim and i > 0:\n",
    "                t_prev = step_indices[i - 1]\n",
    "                alpha_bar_t = self.alpha_bar[t]\n",
    "                alpha_bar_t_prev = self.alpha_bar[t_prev]\n",
    "                sigma = eta * torch.sqrt((1 - alpha_bar_t_prev) / (1 - alpha_bar_t) * (1 - alpha_bar_t / alpha_bar_t_prev))\n",
    "                x_t = torch.where(\n",
    "                    torch.rand_like(x_t.float()) < sigma,\n",
    "                    torch.randint(0, self.vocab_size, x_t.shape, device=device),\n",
    "                    x_t_new\n",
    "                )\n",
    "            else:\n",
    "                x_t = x_t_new\n",
    "\n",
    "        return x_t\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "# Dataset\n",
    "class Text2MusicDataset(Dataset):\n",
    "    def __init__(self, configs, captions, remi_tokenizer, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        self.captions = captions\n",
    "        self.dataset_path = configs[\"raw_data\"][\"dataset_folder\"]\n",
    "        self.remi_tokenizer = remi_tokenizer\n",
    "        self.t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        self.decoder_max_sequence_length = configs[\"model\"][\"text2midi_model\"][\"decoder_max_sequence_length\"]\n",
    "        self.num_steps = configs[\"model\"][\"text2midi_model\"][\"num_diffusion_steps\"]\n",
    "        self.vocab_size = len(remi_tokenizer)\n",
    "\n",
    "        self.betas = cosine_beta_schedule(self.num_steps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bar = torch.cumprod(self.alphas, dim=0)\n",
    "        self.Q = torch.ones(self.vocab_size, self.vocab_size) / self.vocab_size\n",
    "\n",
    "        valid_captions = []\n",
    "        for cap in captions:\n",
    "            if not isinstance(cap, dict) or \"caption\" not in cap or \"location\" not in cap:\n",
    "                logger.warning(f\"Invalid caption format: {cap}\")\n",
    "                continue\n",
    "            midi_path = os.path.join(self.dataset_path, cap[\"location\"])\n",
    "            if os.path.exists(midi_path):\n",
    "                valid_captions.append(cap)\n",
    "            else:\n",
    "                logger.warning(f\"MIDI file not found: {midi_path}\")\n",
    "        self.captions = valid_captions\n",
    "        if not self.captions:\n",
    "            raise ValueError(\"No valid captions found after validation\")\n",
    "        logger.info(f\"Dataset size after validation: {len(self.captions)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.captions[idx][\"caption\"]\n",
    "        midi_filepath = os.path.join(self.dataset_path, self.captions[idx][\"location\"])\n",
    "\n",
    "        try:\n",
    "            tokens = self.remi_tokenizer(midi_filepath)\n",
    "            tokenized_midi = ([self.remi_tokenizer[\"BOS_None\"]] + tokens.ids +\n",
    "                             [self.remi_tokenizer[\"EOS_None\"]]) if tokens.ids else \\\n",
    "                            [self.remi_tokenizer[\"BOS_None\"], self.remi_tokenizer[\"EOS_None\"]]\n",
    "            tokenized_midi = torch.tensor(tokenized_midi)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error tokenizing MIDI file {midi_filepath}: {str(e)}\")\n",
    "            tokenized_midi = torch.tensor([self.remi_tokenizer[\"BOS_None\"], self.remi_tokenizer[\"EOS_None\"]])\n",
    "\n",
    "        if len(tokenized_midi) < self.decoder_max_sequence_length:\n",
    "            x_0 = F.pad(tokenized_midi, (0, self.decoder_max_sequence_length - len(tokenized_midi))).long()\n",
    "        else:\n",
    "            x_0 = tokenized_midi[:self.decoder_max_sequence_length].long()\n",
    "\n",
    "        t = torch.randint(0, self.num_steps, (1,)).item()\n",
    "        alpha_bar_t = self.alpha_bar[t]\n",
    "        Q_t = alpha_bar_t * torch.eye(self.vocab_size) + (1 - alpha_bar_t) * self.Q\n",
    "        probs = F.one_hot(x_0, num_classes=self.vocab_size).float() @ Q_t\n",
    "        x_t = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "        inputs = self.t5_tokenizer(caption, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        return (caption, inputs[\"input_ids\"].squeeze(0), inputs[\"attention_mask\"].squeeze(0),\n",
    "                x_t.long(), t, x_0.long())\n",
    "\n",
    "def collate_fn(batch):\n",
    "    captions = [item[0] for item in batch]\n",
    "    input_ids = nn.utils.rnn.pad_sequence([item[1] for item in batch], batch_first=True, padding_value=0)\n",
    "    attention_mask = nn.utils.rnn.pad_sequence([item[2] for item in batch], batch_first=True, padding_value=0)\n",
    "    x_t = nn.utils.rnn.pad_sequence([item[3] for item in batch], batch_first=True, padding_value=0)\n",
    "    t = torch.tensor([item[4] for item in batch])\n",
    "    x_0 = nn.utils.rnn.pad_sequence([item[5] for item in batch], batch_first=True, padding_value=0)\n",
    "    return captions, input_ids, attention_mask, x_t, t, x_0\n",
    "\n",
    "# Training Function\n",
    "def train_model(configs):\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=configs[\"training\"][\"text2midi_model\"][\"gradient_accumulation_steps\"],\n",
    "        mixed_precision=\"fp16\" if configs[\"training\"][\"text2midi_model\"][\"fp16_precision\"] else \"no\"\n",
    "    )\n",
    "\n",
    "    output_dir = configs[\"training\"][\"text2midi_model\"][\"output_dir\"]\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        if configs[\"training\"][\"text2midi_model\"][\"with_tracking\"]:\n",
    "            wandb.init(project=\"Text-2-Midi\")\n",
    "\n",
    "    vocab_path = os.path.join(configs[\"artifact_folder\"], \"vocab_remi.pkl\")\n",
    "    if not os.path.exists(vocab_path):\n",
    "        raise FileNotFoundError(f\"Vocabulary file not found: {vocab_path}\")\n",
    "    with open(vocab_path, \"rb\") as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "\n",
    "    caption_path = configs[\"raw_data\"][\"caption_dataset_path\"]\n",
    "    if not os.path.exists(caption_path):\n",
    "        raise FileNotFoundError(f\"Caption file not found: {caption_path}\")\n",
    "    with jsonlines.open(caption_path) as reader:\n",
    "        captions = list(reader)[:2000]  # Limit to 2000 captions\n",
    "    if not captions:\n",
    "        raise ValueError(\"No captions loaded from file\")\n",
    "    train_captions, temp_captions = train_test_split(captions, test_size=0.2, random_state=42)\n",
    "    val_captions, test_captions = train_test_split(temp_captions, test_size=0.5, random_state=42)\n",
    "    logger.info(f\"Train: {len(train_captions)}, Val: {len(val_captions)}, Test: {len(test_captions)}\")\n",
    "\n",
    "    train_dataset = Text2MusicDataset(configs, train_captions, remi_tokenizer=tokenizer, mode=\"train\")\n",
    "    val_dataset = Text2MusicDataset(configs, val_captions, remi_tokenizer=tokenizer, mode=\"val\")\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=configs[\"training\"][\"text2midi_model\"][\"per_device_train_batch_size\"],\n",
    "        shuffle=True, num_workers=4, collate_fn=collate_fn, drop_last=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=configs[\"training\"][\"text2midi_model\"][\"per_device_train_batch_size\"],\n",
    "        shuffle=False, num_workers=4, collate_fn=collate_fn, drop_last=False\n",
    "    )\n",
    "\n",
    "    model = DiscreteDiffusionModel(\n",
    "        vocab_size=len(tokenizer),\n",
    "        d_model=configs[\"model\"][\"text2midi_model\"][\"decoder_d_model\"],\n",
    "        nhead=configs[\"model\"][\"text2midi_model\"][\"decoder_num_heads\"],\n",
    "        num_layers=configs[\"model\"][\"text2midi_model\"][\"decoder_num_layers\"],\n",
    "        dim_feedforward=configs[\"model\"][\"text2midi_model\"][\"decoder_intermediate_size\"],\n",
    "        num_steps=configs[\"model\"][\"text2midi_model\"][\"num_diffusion_steps\"],\n",
    "        device=accelerator.device\n",
    "    )\n",
    "\n",
    "    best_model_path = os.path.join(output_dir, \"best_model.bin\")\n",
    "    if os.path.exists(best_model_path):\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=accelerator.device))\n",
    "        logger.info(f\"Loaded checkpoint from {best_model_path}\")\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=configs[\"training\"][\"text2midi_model\"][\"learning_rate\"],\n",
    "        weight_decay=configs[\"training\"][\"text2midi_model\"][\"weight_decay\"]\n",
    "    )\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / configs[\"training\"][\"text2midi_model\"][\"gradient_accumulation_steps\"])\n",
    "    max_train_steps = configs[\"training\"][\"text2midi_model\"][\"epochs\"] * num_update_steps_per_epoch\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=configs[\"training\"][\"text2midi_model\"][\"lr_scheduler_type\"],\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=configs[\"training\"][\"text2midi_model\"][\"num_warmup_steps\"],\n",
    "        num_training_steps=max_train_steps\n",
    "    )\n",
    "\n",
    "    model, optimizer, lr_scheduler, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "        model, optimizer, lr_scheduler, train_dataloader, val_dataloader\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    progress_bar = tqdm(range(max_train_steps), desc=\"Training\", disable=not accelerator.is_local_main_process)\n",
    "    completed_steps = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(configs[\"training\"][\"text2midi_model\"][\"epochs\"]):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_accuracy = 0\n",
    "        total_train_perplexity = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            captions, encoder_input, attention_mask, x_t, t, x_0 = batch\n",
    "            with accelerator.accumulate(model):\n",
    "                logits = model(x_t, t, encoder_input, attention_mask)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), x_0.view(-1))\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    logger.warning(f\"Invalid loss at epoch {epoch+1}, skipping\")\n",
    "                    continue\n",
    "\n",
    "                total_train_loss += loss.detach().float()\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                total_train_accuracy += (preds == x_0).float().mean().item()\n",
    "                total_train_perplexity += torch.exp(loss.detach()).item()\n",
    "                num_batches += 1\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), configs[\"training\"][\"text2midi_model\"][\"max_grad_norm\"])\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "        if num_batches == 0:\n",
    "            logger.warning(f\"No valid batches processed in epoch {epoch+1}\")\n",
    "            continue\n",
    "\n",
    "        avg_loss = total_train_loss / num_batches\n",
    "        avg_accuracy = total_train_accuracy / num_batches\n",
    "        avg_perplexity = total_train_perplexity / num_batches\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                _, encoder_input, attention_mask, x_t, t, x_0 = batch\n",
    "                logits = model(x_t, t, encoder_input, attention_mask)\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), x_0.view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        avg_val_loss = total_val_loss / val_batches if val_batches > 0 else float(\"inf\")\n",
    "\n",
    "        # Evaluate metrics on validation subset\n",
    "        num_eval_samples = min(10, len(val_dataset))\n",
    "        eval_indices = random.sample(range(len(val_dataset)), num_eval_samples)\n",
    "        eval_metrics = {\"CR\": [], \"TB\": [], \"TBT\": [], \"CK\": [], \"CKD\": [], \"CLAP\": []}\n",
    "\n",
    "        for idx in eval_indices:\n",
    "            caption, input_ids, attention_mask, _, _, _ = val_dataset[idx]\n",
    "            input_ids = input_ids.unsqueeze(0).to(accelerator.device)\n",
    "            attention_mask = attention_mask.unsqueeze(0).to(accelerator.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                generated_tokens = model.sample(\n",
    "                    input_ids, attention_mask,\n",
    "                    seq_len=configs[\"model\"][\"text2midi_model\"][\"decoder_max_sequence_length\"],\n",
    "                    num_steps=configs[\"model\"][\"text2midi_model\"][\"ddim_steps\"],\n",
    "                    ddim=True\n",
    "                )\n",
    "            generated_tokens = generated_tokens[0].cpu().numpy().tolist()\n",
    "\n",
    "            if generated_tokens[0] == tokenizer[\"BOS_None\"]:\n",
    "                generated_tokens = generated_tokens[1:]\n",
    "            if generated_tokens[-1] == tokenizer[\"EOS_None\"]:\n",
    "                generated_tokens = generated_tokens[:-1]\n",
    "\n",
    "            # Compression Ratio\n",
    "            cr = compression_ratio(generated_tokens)\n",
    "            eval_metrics[\"CR\"].append(cr)\n",
    "\n",
    "            # Tempo and Key Metrics\n",
    "            tempo_term, target_key = parse_caption(caption)\n",
    "            target_tempo_bin = map_tempo_term_to_bin(tempo_term)\n",
    "            generated_tempo_bin = extract_tempo_bin(generated_tokens, tokenizer)\n",
    "\n",
    "            if generated_tempo_bin is not None and target_tempo_bin is not None:\n",
    "                tb = 1 if generated_tempo_bin == target_tempo_bin else 0\n",
    "                tbt = 1 if abs(generated_tempo_bin - target_tempo_bin) <= 1 else 0\n",
    "            else:\n",
    "                tb = tbt = 0\n",
    "            eval_metrics[\"TB\"].append(tb)\n",
    "            eval_metrics[\"TBT\"].append(tbt)\n",
    "\n",
    "            # Generate MIDI for Key and CLAP\n",
    "            try:\n",
    "                with tempfile.NamedTemporaryFile(suffix=\".mid\", delete=True) as temp_midi:\n",
    "                    midi_score = tokenizer.decode(generated_tokens)\n",
    "                    midi_score.dump(temp_midi.name)\n",
    "                    detected_key = detect_key(temp_midi.name)\n",
    "                    ck = 1 if detected_key and target_key and detected_key.lower() == target_key.lower() else 0\n",
    "                    eval_metrics[\"CK\"].append(ck)\n",
    "                    eval_metrics[\"CKD\"].append(ck)  # Same as CK for simplicity\n",
    "                    pm = pretty_midi.PrettyMIDI(temp_midi.name)\n",
    "                    audio = pm.synthesize(fs=44100)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error in eval sample {idx}: {str(e)}\")\n",
    "                eval_metrics[\"CK\"].append(0)\n",
    "                eval_metrics[\"CKD\"].append(0)\n",
    "                eval_metrics[\"CLAP\"].append(0)\n",
    "\n",
    "        avg_metrics = {k: sum(v) / len(v) for k, v in eval_metrics.items() if v}\n",
    "\n",
    "        # Log metrics\n",
    "        if configs[\"training\"][\"text2midi_model\"][\"with_tracking\"] and accelerator.is_main_process:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": avg_loss,\n",
    "                \"train_accuracy\": avg_accuracy,\n",
    "                \"train_perplexity\": avg_perplexity,\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_CR\": avg_metrics.get(\"CR\", 0),\n",
    "                \"val_TB\": avg_metrics.get(\"TB\", 0),\n",
    "                \"val_TBT\": avg_metrics.get(\"TBT\", 0),\n",
    "                \"val_CK\": avg_metrics.get(\"CK\", 0),\n",
    "                \"val_CKD\": avg_metrics.get(\"CKD\", 0),\n",
    "                \"val_CLAP\": avg_metrics.get(\"CLAP\", 0)\n",
    "            })\n",
    "\n",
    "        logger.info(f\"Epoch {epoch+1}: Train Loss={avg_loss:.4f}, Accuracy={avg_accuracy:.4f}, \"\n",
    "                    f\"Perplexity={avg_perplexity:.4f}, Val Loss={avg_val_loss:.4f}, \"\n",
    "                    f\"CR={avg_metrics.get('CR', 0):.4f}, TB={avg_metrics.get('TB', 0):.4f}, \"\n",
    "                    f\"TBT={avg_metrics.get('TBT', 0):.4f}, CK={avg_metrics.get('CK', 0):.4f}, \"\n",
    "                    f\"CLAP={avg_metrics.get('CLAP', 0):.4f}\")\n",
    "\n",
    "        # Save checkpoints\n",
    "        if (epoch + 1) % configs[\"training\"][\"text2midi_model\"][\"save_every\"] == 0 and accelerator.is_main_process:\n",
    "            checkpoint_path = os.path.join(output_dir, f\"checkpoint_epoch_{epoch+1}.bin\")\n",
    "            torch.save(accelerator.unwrap_model(model).state_dict(), checkpoint_path)\n",
    "            logger.info(f\"Saved checkpoint at {checkpoint_path}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss and accelerator.is_main_process:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(accelerator.unwrap_model(model).state_dict(), best_model_path)\n",
    "            logger.info(f\"Saved best model at {best_model_path}\")\n",
    "\n",
    "    accelerator.end_training()\n",
    "\n",
    "# Vocabulary Building\n",
    "def build_vocab_remi(configs):\n",
    "    TOKENIZER_PARAMS = {\n",
    "        \"pitch_range\": (21, 109),\n",
    "        \"beat_res\": {(0, 1): 12, (1, 2): 4, (2, 4): 2, (4, 8): 1},\n",
    "        \"num_velocities\": 32,\n",
    "        \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n",
    "        \"use_chords\": False,\n",
    "        \"use_rests\": False,\n",
    "        \"use_tempos\": True,\n",
    "        \"use_time_signatures\": True,\n",
    "        \"use_programs\": True,\n",
    "        \"num_tempos\": 32,\n",
    "        \"tempo_range\": (40, 250),\n",
    "    }\n",
    "    config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "    tokenizer = REMI(config)\n",
    "    logger.info(f\"Vocabulary length: {tokenizer.vocab_size}\")\n",
    "    vocab_path = os.path.join(configs[\"artifact_folder\"], \"vocab_remi.pkl\")\n",
    "    os.makedirs(configs[\"artifact_folder\"], exist_ok=True)\n",
    "    with open(vocab_path, \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    logger.info(f\"Vocabulary saved to {vocab_path}\")\n",
    "\n",
    "# Inference Function\n",
    "def generate_midi(model, tokenizer, t5_tokenizer, caption, seq_len, output_dir, device, ddim=True, num_steps=100):\n",
    "    model.eval()\n",
    "    inputs = t5_tokenizer(caption, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokens = model.sample(input_ids, attention_mask, seq_len, num_steps=num_steps, ddim=ddim)\n",
    "\n",
    "    token_ids = tokens[0].cpu().numpy().tolist()\n",
    "    if token_ids[0] == tokenizer[\"BOS_None\"]:\n",
    "        token_ids = token_ids[1:]\n",
    "    if token_ids[-1] == tokenizer[\"EOS_None\"]:\n",
    "        token_ids = token_ids[:-1]\n",
    "\n",
    "    try:\n",
    "        midi_score = tokenizer.decode(token_ids)\n",
    "        output_path = os.path.join(output_dir, f\"generated_{int(time.time())}.mid\")\n",
    "        midi_score.dump_midi(output_path)\n",
    "        logger.info(f\"Generated MIDI saved to {output_path}\")\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting tokens to MIDI: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main(mode=\"infer\"):\n",
    "    if mode == \"train\":\n",
    "        train_model(CONFIG)\n",
    "    elif mode == \"build_vocab_remi\":\n",
    "        build_vocab_remi(CONFIG)\n",
    "    elif mode == \"infer\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        with open(os.path.join(CONFIG[\"artifact_folder\"], \"vocab_remi.pkl\"), \"rb\") as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        model = DiscreteDiffusionModel(\n",
    "            vocab_size=len(tokenizer),\n",
    "            d_model=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_d_model\"],\n",
    "            nhead=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_num_heads\"],\n",
    "            num_layers=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_num_layers\"],\n",
    "            dim_feedforward=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_intermediate_size\"],\n",
    "            num_steps=CONFIG[\"model\"][\"text2midi_model\"][\"num_diffusion_steps\"],\n",
    "            device=device\n",
    "        ).to(device)\n",
    "        checkpoint_path = os.path.join(CONFIG[\"training\"][\"text2midi_model\"][\"output_dir\"], \"best_model.bin\")\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "        t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        caption = \"A melodic electronic composition with classical influences, featuring a string ensemble, trumpet, brass section, synth strings, and drums. Set in F# minor with a 4/4 time signature, it moves at an Allegro tempo. The mood evokes a cinematic, spacious, and epic atmosphere while maintaining a sense of relaxation.\"\n",
    "        generate_midi(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            t5_tokenizer=t5_tokenizer,\n",
    "            caption=caption,\n",
    "            seq_len=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_max_sequence_length\"],\n",
    "            output_dir=CONFIG[\"training\"][\"text2midi_model\"][\"output_dir\"],\n",
    "            device=device,\n",
    "            num_steps=200,\n",
    "            ddim=True\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mode: {mode}. Choose 'train', 'build_vocab_remi', or 'infer'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = os.getenv(\"MODE\", \"infer\")\n",
    "    main(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57904c47",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rm -rf /kaggle/working/saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df2933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T15:58:38.726386Z",
     "iopub.status.busy": "2025-05-17T15:58:38.725106Z",
     "iopub.status.idle": "2025-05-17T15:58:42.414528Z",
     "shell.execute_reply": "2025-05-17T15:58:42.413763Z",
     "shell.execute_reply.started": "2025-05-17T15:58:38.726352Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install flask flask-cors torch transformers mido music21 pretty_midi miditok pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8085d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T16:29:37.192304Z",
     "iopub.status.busy": "2025-05-17T16:29:37.192013Z",
     "iopub.status.idle": "2025-05-17T16:29:37.307794Z",
     "shell.execute_reply": "2025-05-17T16:29:37.307228Z",
     "shell.execute_reply.started": "2025-05-17T16:29:37.192287Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "ngrok.set_auth_token(UserSecretsClient().get_secret(\"ngrok_authtoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ed26a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T16:29:40.338019Z",
     "iopub.status.busy": "2025-05-17T16:29:40.337394Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import base64\n",
    "import torch\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "from transformers import T5Tokenizer\n",
    "from pyngrok import ngrok\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize model, tokenizer, and device\n",
    "try:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Load REMI tokenizer\n",
    "    vocab_path = \"/kaggle/working/artifacts/vocab_remi.pkl\"  # Adjust path\n",
    "    if not os.path.exists(vocab_path):\n",
    "        raise FileNotFoundError(f\"Vocabulary file not found: {vocab_path}\")\n",
    "    with open(vocab_path, \"rb\") as f:\n",
    "        remi_tokenizer = pickle.load(f)\n",
    "    logger.info(\"REMI tokenizer loaded\")\n",
    "\n",
    "    # Load T5 tokenizer\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    logger.info(\"T5 tokenizer loaded\")\n",
    "\n",
    "    # Initialize and load model\n",
    "    model = DiscreteDiffusionModel(\n",
    "        vocab_size=len(remi_tokenizer),\n",
    "        d_model=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_d_model\"],\n",
    "        nhead=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_num_heads\"],\n",
    "        num_layers=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_num_layers\"],\n",
    "        dim_feedforward=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_intermediate_size\"],\n",
    "        num_steps=CONFIG[\"model\"][\"text2midi_model\"][\"num_diffusion_steps\"],\n",
    "        device=device\n",
    "    ).to(device)\n",
    "    checkpoint_path = \"/kaggle/working/saved/best_model.bin\"  # Adjust path\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Model checkpoint not found: {checkpoint_path}\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.eval()\n",
    "    logger.info(\"Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize model or tokenizers: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "@app.route('/generate-midi', methods=['POST'])\n",
    "def generate_midi_endpoint():\n",
    "    data = request.get_json()\n",
    "    prompt = data.get('prompt')\n",
    "\n",
    "    if not prompt:\n",
    "        return jsonify({\"error\": \"Prompt is required\"}), 400\n",
    "\n",
    "    try:\n",
    "        # Generate MIDI\n",
    "        output_path = generate_midi(\n",
    "            model=model,\n",
    "            tokenizer=remi_tokenizer,\n",
    "            t5_tokenizer=t5_tokenizer,\n",
    "            caption=prompt,\n",
    "            seq_len=CONFIG[\"model\"][\"text2midi_model\"][\"decoder_max_sequence_length\"],\n",
    "            output_dir=\"/kaggle/working\",\n",
    "            device=device,\n",
    "            num_steps=200,\n",
    "            ddim=True\n",
    "        )\n",
    "\n",
    "        if not output_path or not os.path.exists(output_path):\n",
    "            return jsonify({\"error\": \"Failed to generate MIDI file\"}), 500\n",
    "\n",
    "        # Read and encode MIDI\n",
    "        with open(output_path, 'rb') as midi_file:\n",
    "            midi_base64 = base64.b64encode(midi_file.read()).decode('utf-8')\n",
    "\n",
    "        # Clean up\n",
    "        try:\n",
    "            os.remove(output_path)\n",
    "            logger.info(f\"Cleaned up: {output_path}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to clean up {output_path}: {str(e)}\")\n",
    "\n",
    "        return jsonify({\n",
    "            \"midi\": {\n",
    "                \"data\": midi_base64,\n",
    "                \"mimetype\": \"audio/midi\"\n",
    "            }\n",
    "        }), 200\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating MIDI: {str(e)}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Start ngrok to expose the Flask app\n",
    "    public_url = ngrok.connect(5000).public_url\n",
    "    logger.info(f\"ngrok tunnel opened at {public_url}\")\n",
    "    \n",
    "    # Update Flask to run on port 5000\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.293817,
   "end_time": "2025-05-17T16:38:44.956561",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-17T16:38:40.662744",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "34b0d08a802c47d18b9ba4a6b9cef386": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61392bbaa5ff4262bfc69c3d72784859",
      "placeholder": "​",
      "style": "IPY_MODEL_85de74ddceb14bfe93913e5b45ca97ea",
      "value": " 119/526200 [02:22&lt;131:04:51,  1.11it/s, Loss=6.43]"
     }
    },
    "61392bbaa5ff4262bfc69c3d72784859": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "783099deef6c41e9bab86d8b3eb01bf6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85de74ddceb14bfe93913e5b45ca97ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a18cf45f7a646df9d6bbae753643923": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9a73d1896a1c458eaeb755050538bdf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc755fd031ac4299abc9787f51ced615",
      "placeholder": "​",
      "style": "IPY_MODEL_f35b19c65e6d4aa696045b09b58dabf2",
      "value": "Training:   0%"
     }
    },
    "d31fd338fe7141898dbf2cfbd3b4187d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee8d342bb98b44549bac06ddb14dd198",
      "max": 526200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a18cf45f7a646df9d6bbae753643923",
      "value": 119
     }
    },
    "ec5ae9639467447d8b99b687054508e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9a73d1896a1c458eaeb755050538bdf2",
       "IPY_MODEL_d31fd338fe7141898dbf2cfbd3b4187d",
       "IPY_MODEL_34b0d08a802c47d18b9ba4a6b9cef386"
      ],
      "layout": "IPY_MODEL_783099deef6c41e9bab86d8b3eb01bf6"
     }
    },
    "ee8d342bb98b44549bac06ddb14dd198": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f35b19c65e6d4aa696045b09b58dabf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc755fd031ac4299abc9787f51ced615": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
